{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9978f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Python\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73534c3",
   "metadata": {},
   "source": [
    "# Loading the Dataset and tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ff125ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Spam SMS Collection.txt', sep='\\t', names=['label', 'message'])\n",
    "\n",
    "# Map labels to numerical values\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07484b20",
   "metadata": {},
   "source": [
    "# Using the wordnet and calculating the similarity for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ae03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_semantic_similarity(text, reference_words):\n",
    "    score = 0\n",
    "    word_synsets = [wn.synsets(word) for word in text.lower().split()]\n",
    "    ref_synsets = [wn.synsets(word) for word in reference_words]\n",
    "    for word in word_synsets:\n",
    "        for ref in ref_synsets:\n",
    "            if word and ref:\n",
    "                word_best = max((word[0].path_similarity(ref_word), ref_word) for ref_word in ref if word[0].path_similarity(ref_word) is not None)\n",
    "                score += word_best[0] if word_best[0] is not None else 0\n",
    "    return score / len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88082c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and preparing BERT inputs\n",
    "def prepare_texts(texts):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    semantic_scores = []\n",
    "\n",
    "    spam_words = ['free', 'win', 'winner', 'urgent', 'alert', 'claim', 'prize', 'congratulations', 'guaranteed', 'offer']\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        semantic_score = calculate_semantic_similarity(text, spam_words)\n",
    "        \n",
    "        input_ids.append(inputs['input_ids'][0])\n",
    "        attention_masks.append(inputs['attention_mask'][0])\n",
    "        semantic_scores.append([semantic_score])\n",
    "        \n",
    "    return np.array(input_ids), np.array(attention_masks), np.array(semantic_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94bc0873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "D:\\Python\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Python\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "X_ids, X_masks, X_semantics = prepare_texts(df['message'])\n",
    "y = df['label'].values\n",
    "\n",
    "# Split the dataset\n",
    "X_train_ids, X_test_ids, X_train_masks, X_test_masks, X_train_semantics, X_test_semantics, y_train, y_test = train_test_split(X_ids, X_masks, X_semantics, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load BERT model\n",
    "bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Freeze BERT layers\n",
    "for layer in bert.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6118387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "semantic_input = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"semantic_score\")\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask=input_mask)[1]\n",
    "concat = tf.keras.layers.concatenate([embeddings, semantic_input])\n",
    "\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(concat)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "y = tf.keras.layers.Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32cbf47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Python\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=[input_ids, input_mask, semantic_input], outputs=y)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19962405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "112/112 [==============================] - 448s 4s/step - loss: 0.0757 - accuracy: 0.9753 - val_loss: 0.1067 - val_accuracy: 0.9608\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 449s 4s/step - loss: 0.0786 - accuracy: 0.9734 - val_loss: 0.1019 - val_accuracy: 0.9720\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 448s 4s/step - loss: 0.0745 - accuracy: 0.9750 - val_loss: 0.1062 - val_accuracy: 0.9697\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 454s 4s/step - loss: 0.0682 - accuracy: 0.9776 - val_loss: 0.1187 - val_accuracy: 0.9664\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 448s 4s/step - loss: 0.0793 - accuracy: 0.9728 - val_loss: 0.0865 - val_accuracy: 0.9720\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 446s 4s/step - loss: 0.0651 - accuracy: 0.9773 - val_loss: 0.0854 - val_accuracy: 0.9776\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 449s 4s/step - loss: 0.0687 - accuracy: 0.9778 - val_loss: 0.0930 - val_accuracy: 0.9675\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 448s 4s/step - loss: 0.0579 - accuracy: 0.9815 - val_loss: 0.1350 - val_accuracy: 0.9641\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 449s 4s/step - loss: 0.0859 - accuracy: 0.9714 - val_loss: 0.0889 - val_accuracy: 0.9742\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 450s 4s/step - loss: 0.0561 - accuracy: 0.9832 - val_loss: 0.0883 - val_accuracy: 0.9697\n",
      "35/35 [==============================] - 95s 3s/step - loss: 0.0626 - accuracy: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06259728968143463, 0.9829596281051636]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit([X_train_ids, X_train_masks, X_train_semantics], y_train, batch_size=32, validation_split=0.2, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate([X_test_ids, X_test_masks, X_test_semantics], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de031e",
   "metadata": {},
   "source": [
    "# Function for Predicting a message( Spam or Ham )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7121aefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 104ms/step\n",
      "The message is classified as Spam with a probability of 0.9982\n"
     ]
    }
   ],
   "source": [
    "def predict_spam(text):\n",
    "    # Prepare the text inputs for BERT\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf',\n",
    "    )\n",
    "    \n",
    "    # Calculate semantic similarity\n",
    "    spam_words = ['free', 'win', 'winner', 'urgent', 'alert', 'claim', 'prize', 'congratulations', 'guaranteed', 'offer']\n",
    "    semantic_score = calculate_semantic_similarity(text, spam_words)\n",
    "    \n",
    "    # Prepare the input dictionary to match the model's input format\n",
    "    input_dict = {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'semantic_score': np.array([[semantic_score]])  # Ensure it's in the same shape as during training\n",
    "    }\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(input_dict)[0]\n",
    "    \n",
    "    # Determine the class and probability\n",
    "    class_id = 1 if prediction >= 0.5 else 0\n",
    "    probability = prediction[0]\n",
    "    \n",
    "    if class_id == 1:\n",
    "        return \"Spam\", probability\n",
    "    else:\n",
    "        return \"Ham\", probability\n",
    "\n",
    "# Example usage\n",
    "text = \"you've won £1000 cash! To get your money, text ‘CLAIM’ to 81010 now! Cost £3.00 per msg.\"\n",
    "text=text.lower()\n",
    "classification, probability = predict_spam(text)\n",
    "print(f\"The message is classified as {classification} with a probability of {probability:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a69443b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 103ms/step\n",
      "The message is classified as Spam with a probability of 0.9401\n"
     ]
    }
   ],
   "source": [
    "text = \"Congratulations! You've been selected to win a free iPhone! Click here to claim your prize now!\"\n",
    "text=text.lower()\n",
    "classification, probability = predict_spam(text)\n",
    "print(f\"The message is classified as {classification} with a probability of {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aad54951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step\n",
      "The message is classified as Ham with a probability of 0.0141\n"
     ]
    }
   ],
   "source": [
    "text = \"Hey, are we still on for dinner tonight? Looking forward to catching up!\"\n",
    "text=text.lower()\n",
    "classification, probability = predict_spam(text)\n",
    "print(f\"The message is classified as {classification} with a probability of {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "693867ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 107ms/step\n",
      "The message is classified as Spam with a probability of 0.9895\n"
     ]
    }
   ],
   "source": [
    "text = \"URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot!\"\n",
    "text=text.lower()\n",
    "classification, probability = predict_spam(text)\n",
    "print(f\"The message is classified as {classification} with a probability of {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac94e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 103ms/step\n",
      "The message is classified as Ham with a probability of 0.0047\n"
     ]
    }
   ],
   "source": [
    "text = \"Can you please send me the directions to the park? I forgot to save them last time.\"\n",
    "text=text.lower()\n",
    "classification, probability = predict_spam(text)\n",
    "print(f\"The message is classified as {classification} with a probability of {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12c12b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 135ms/step\n",
      "The message is classified as Ham with a probability of 0.0251\n"
     ]
    }
   ],
   "source": [
    "text = \"Just finished the meeting, I'll call you in 5 minutes.\"\n",
    "text=text.lower()\n",
    "classification, probability = predict_spam(text)\n",
    "print(f\"The message is classified as {classification} with a probability of {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28b4a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#Save the model\n",
    "model.save('models/NLP_SPAM_model.h5')# saves to HDF5 file\n",
    "model.save_weights('models/NLP_SPAM_weights.h5')\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf2a5f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 162ms/step\n",
      "The message is classified as Spam with a probability of 0.5737\n"
     ]
    }
   ],
   "source": [
    "text = '''Hey there! Are you tired of the same old routine? Looking for something new and exciting to spice up your life? Well, look no further! Introducing the amazing, the incredible, the life-changing product that will revolutionize the way you live! Say goodbye to boredom and hello to endless possibilities with our revolutionary solution.\n",
    "\n",
    "But wait, there's more! If you act now, you'll receive a special discount that's too good to pass up. This offer won't last long, so don't miss out on the opportunity to transform your life for the better. Whether you're at home, at work, or on the go, our product will enhance every aspect of your daily life.\n",
    "\n",
    "Thousands of satisfied customers can't be wrong! Join the ranks of the happy and fulfilled individuals who have already experienced the incredible benefits of our product. Don't hesitate, take the leap and unlock a world of excitement and joy!\n",
    "\n",
    "So what are you waiting for? Take the first step towards a brighter future and seize this amazing opportunity today. Remember, this is a limited-time offer, so act now and start living your best life!'''\n",
    "text=text.lower()\n",
    "classification, probability = predict_spam(text)\n",
    "print(f\"The message is classified as {classification} with a probability of {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56c136dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 129ms/step\n",
      "The message is classified as Ham with a probability of 0.1773\n"
     ]
    }
   ],
   "source": [
    "text = '''Hey there! Just wanted to remind you about the upcoming team meeting on Monday at 10 am. We'll be discussing the latest project updates and setting goals for the next phase. Your input will be valuable, so make sure to come prepared with any insights or suggestions you might have. Looking forward to a productive session!\n",
    "\n",
    "\n",
    "In other news, the company picnic is just around the corner, and we're all excited to spend a fun day outdoors. Don't forget to sign up for the potluck and let us know what dish you'll be bringing. It's always a great opportunity to bond with colleagues outside of work.\n",
    "\n",
    "\n",
    "On a personal note, I wanted to share that I recently read a fascinating book that I think you'd enjoy. It's a gripping mystery novel with an unexpected twist at the end. Let me know if you'd like to borrow it sometime.\n",
    "\n",
    "\n",
    "Lastly, I hope you have a fantastic weekend ahead! Take some time to relax and recharge for the week ahead. If there's anything you need assistance with, feel free to reach out. Have a great day'''\n",
    "text=text.lower()\n",
    "classification, probability = predict_spam(text)\n",
    "print(f\"The message is classified as {classification} with a probability of {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b3dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
